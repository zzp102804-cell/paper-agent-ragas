import os
import tempfile
import math
import streamlit as st
from datasets import Dataset
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_community.retrievers import BM25Retriever
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_community.vectorstores import Chroma
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.tools.retriever import create_retriever_tool
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.prebuilt import create_react_agent
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.run_config import RunConfig
# ================= 1. é¡µé¢é…ç½® =================
st.set_page_config(page_title="å…¨èƒ½æ–‡çŒ® Agent", layout="wide", page_icon="ğŸ“")
# ================= 2. æ ·å¼ä¼˜åŒ– (CSS) =================
st.markdown("""
<style>
    /* æ–‡ä»¶ä¸Šä¼ åŒºçš„æ–‡å­—æç¤º */
    [data-testid="stFileUploaderDropzoneInstructions"] > div > span {visibility: hidden; height: 0;}
    [data-testid="stFileUploaderDropzoneInstructions"] > div > small {visibility: hidden; height: 0;}
    [data-testid="stFileUploaderDropzoneInstructions"] > div::before {
        content: "è¯·å°†æ–‡ä»¶æ‹–æ‹½è‡³æ­¤";
        visibility: visible;
        display: block;
        font-size: 1rem;
        margin-bottom: 0.5rem;
    }
    [data-testid="stFileUploaderDropzoneInstructions"] > div::after {
        content: "å•ä¸ªæ–‡ä»¶é™åˆ¶ 200MB â€¢ PDF, DOCX, TXT";
        visibility: visible;
        display: block;
        font-size: 0.8rem;
        color: #808495;
    }
    [data-testid="stFileUploader"] button[data-testid="baseButton-secondary"] {
        font-size: 0 !important;
    }
    [data-testid="stFileUploader"] button[data-testid="baseButton-secondary"]::after {
        content: "æµè§ˆæ–‡ä»¶";
        font-size: 1rem !important;
        visibility: visible;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“ å…¨èƒ½æ–‡çŒ® Agent")
# ================= 3. ä¾§è¾¹æ  (æ§åˆ¶å°) =================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    api_key = st.text_input("è¯·è¾“å…¥ OpenRouter/OpenAI API Key", type="password")

    uploaded_files = st.file_uploader(
        "ä¸Šä¼ è®ºæ–‡ (æ”¯æŒå¤šä¸ªæ–‡ä»¶)",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )
    st.info("ğŸ’¡ æ¨¡å‹å·²é”å®šä¸º: Chatgpt-5.2")
    if uploaded_files:
        st.write("ğŸ“š **å·²åŠ è½½æ–‡æ¡£åˆ—è¡¨:**")
        for f in uploaded_files:
            st.caption(f"- {f.name}")

    st.divider()
# ================= 4. æ ¸å¿ƒå·¥å…·å®šä¹‰ =================
def save_uploaded_files(uploaded_files):
    file_paths = []
    for uploaded_file in uploaded_files:
        file_extension = os.path.splitext(uploaded_file.name)[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            file_paths.append(tmp_file.name)
    return file_paths

@st.cache_resource
def create_rag_tool(file_paths, api_key):
    """åˆ›å»ºæ£€ç´¢å·¥å…·ï¼ˆåŒ…å«ä¼˜åŒ–åçš„å‚æ•°ï¼šOverlap, Top-K, Flashrankï¼‰"""
    all_docs = []
    for file_path in file_paths:
        if file_path.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith(".docx"):
            loader = Docx2txtLoader(file_path)
        else:
            loader = TextLoader(file_path)
        all_docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)
    splits = splitter.split_documents(all_docs)

    embeddings = OpenAIEmbeddings(
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1"
    )

    vectorstore = Chroma.from_documents(splits, embeddings)
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 30})

    bm25_retriever = BM25Retriever.from_documents(splits)
    bm25_retriever.k = 30

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )

    compressor = FlashrankRerank(top_n=10)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=ensemble_retriever
    )

    tool = create_retriever_tool(
        compression_retriever,
        "search_paper_content",
        "å¿…é¡»ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·æ¥æœç´¢å’ŒæŸ¥è¯¢ç”¨æˆ·ä¸Šä¼ çš„è®ºæ–‡å†…å®¹ã€‚åŒ…å«å…·ä½“æ•°æ®ã€æ¨¡å‹ã€ç»“è®ºç­‰ã€‚"
    )
    return tool

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—å™¨å·¥å…·"""
    try:
        return str(eval(expression))
    except Exception as e:
        return f"è®¡ç®—å‡ºé”™: {e}"

search_tool = DuckDuckGoSearchRun()
# ================= 5. Agent åˆå§‹åŒ– =================
def initialize_agent(rag_tool, api_key):
    llm = ChatOpenAI(
        model="openai/gpt-5.2",
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
    tools = [rag_tool, calculator, search_tool]
    agent = create_react_agent(llm, tools)
    return agent
# ================= 6. Ragas è¯„æµ‹æ¨¡å— =================
def run_real_ragas_evaluation(question, answer, contexts, api_key):
    """é…ç½®ï¼šn=1, temperature=0, timeout=1200s"""
    try:
        data = {
            'question': [question],
            'answer': [answer],
            'contexts': [contexts],
        }
        dataset = Dataset.from_dict(data)

        eval_llm = ChatOpenAI(
            model="openai/gpt-5.2",
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1",
            temperature=0,
            timeout=1200,
            max_retries=3
        )

        eval_embeddings = OpenAIEmbeddings(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )

        my_run_config = RunConfig(
            timeout=1200,
            max_workers=1,
            max_retries=3
        )

        result = evaluate(
            dataset=dataset,
            metrics=[faithfulness, answer_relevancy],
            llm=eval_llm,
            embeddings=eval_embeddings,
            raise_exceptions=False,
            run_config=my_run_config
        )
        return result
    except Exception as e:
        print(f"Ragas å†…éƒ¨æŠ¥é”™: {str(e)}")
        return {"error": str(e)}
# ================= 7. ä¸»é€»è¾‘ =================
if uploaded_files and api_key:
    file_paths = save_uploaded_files(uploaded_files)
    rag_tool = create_rag_tool(file_paths, api_key)

    # Agent åˆå§‹åŒ–
    if "agent_engine" not in st.session_state:
        st.session_state.agent_engine = initialize_agent(rag_tool, api_key)
        st.toast("Multi-Agent ç³»ç»Ÿå·²æ¿€æ´»ï¼", icon="ğŸš€")

    # åˆå§‹åŒ– Session State
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯•ç€é—®æˆ‘ï¼šç»™æˆ‘ç”Ÿæˆè¿™äº›è®ºæ–‡çš„æ–‡çŒ®ç»¼è¿°..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            retrieved_contexts = []

            level4_system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªç”± 'Researcher' (ç ”ç©¶å‘˜) å’Œ 'Writer' (ä½œå®¶) ç»„æˆçš„ Level 4 å­¦æœ¯æ™ºèƒ½ä½“ã€‚

            1. **Researcher é˜¶æ®µ**: 
               - å½“ç”¨æˆ·æé—®æ—¶ï¼Œä½ å¿…é¡»å…ˆè°ƒç”¨å·¥å…· (search_paper_content, DuckDuckGo) è·å–äº‹å®ã€‚
               - ä¸¥ç¦å‡­ç©ºæé€ æ•°æ®ã€‚å¦‚æœæ²¡æœ‰æŸ¥åˆ°ï¼Œå°±è¯´æ²¡æŸ¥åˆ°ã€‚

            2. **Writer é˜¶æ®µ**:
               - æ‹¿åˆ°æ•°æ®åï¼Œä»¥ä¸¥è°¨çš„å­¦æœ¯é£æ ¼ï¼ˆAcademic Toneï¼‰æ’°å†™å›ç­”ã€‚
               - å¼•ç”¨æ•°æ®æ—¶è¦å…·ä½“ã€‚
               - å¦‚æœæ¶‰åŠå¤šæœŸ/å¤šè®ºæ–‡æ•°æ®çš„å¯¹æ¯”ï¼ˆå¦‚ç¬¬ä¸€æœŸ/ç¯‡vsç¬¬äºŒæœŸ/ç¯‡vsç¬¬ä¸‰æœŸ/ç¯‡...ï¼‰ï¼Œ**å¿…é¡»ä½¿ç”¨ Markdown è¡¨æ ¼**è¿›è¡Œå±•ç¤ºï¼Œä»¥ä¾¿äºæˆ‘å¾ˆç›´è§‚åœ°è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

            è¯·ä¸¥æ ¼éµå¾ªï¼šå…ˆæ€è€ƒ -> å†³å®šè°ƒç”¨å“ªä¸ªå·¥å…· -> è·å–ç»“æœ -> æœ€ç»ˆå†™ä½œ çš„æµç¨‹ã€‚
            """

            with st.status("ğŸ•µï¸â€â™‚ï¸ Agent (Researcher & Writer) æ­£åœ¨ååŒ...", expanded=True) as status:
                messages_input = [
                    SystemMessage(content=level4_system_prompt),
                    HumanMessage(content=prompt)
                ]

                event_stream = st.session_state.agent_engine.stream(
                    {"messages": messages_input},
                    stream_mode="values"
                )

                final_answer = ""

                # æµå¼è¾“å‡ºå¤„ç†
                for event in event_stream:
                    if "messages" in event:
                        last_msg = event["messages"][-1]
                        if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                            for tool_call in last_msg.tool_calls:
                                status.write(f"ğŸ”¨ **Researcher**: è°ƒç”¨å·¥å…· `{tool_call['name']}`")
                        elif last_msg.type == 'tool':
                            content = str(last_msg.content)
                            preview = content[:50] + "..."
                            status.write(f"ğŸ“Š **Data Acquired**: {preview}")
                            retrieved_contexts.append(content)
                        elif last_msg.content:
                            if not (hasattr(last_msg, 'tool_calls') and last_msg.tool_calls):
                                final_answer = last_msg.content

                status.update(label="âœ… Writer å†™ä½œå®Œæˆ", state="complete", expanded=False)

            message_placeholder.markdown(final_answer)
            st.session_state.messages.append({"role": "assistant", "content": final_answer})

            # Ragas è¯„æµ‹
            if retrieved_contexts:
                with st.expander("AI ç”Ÿæˆå†…å®¹ Ragas è¯„æµ‹"):
                    st.info("æ­£åœ¨è°ƒç”¨ Ragas åº“è¿›è¡Œç›¸å…³æŒ‡æ ‡è®¡ç®— (Faithfulness & Relevancy)...")
                    ragas_result = run_real_ragas_evaluation(prompt, final_answer, retrieved_contexts, api_key)

                    if isinstance(ragas_result, dict) and "error" in ragas_result:
                        st.error(f"Ragas è¯„æµ‹å‡ºé”™: {ragas_result['error']}")
                    else:
                        df_res = ragas_result.to_pandas()
                        f_val = df_res.iloc[0]['faithfulness']
                        r_val = df_res.iloc[0]['answer_relevancy']

                        def format_score(val):
                            if isinstance(val, float) and math.isnan(val):
                                return None
                            return val * 10

                        f_score = format_score(f_val)
                        r_score = format_score(r_val)

                        c1, c2 = st.columns(2)
                        with c1:
                            if f_score is None:
                                st.warning("ä¿¡åº¦: è¯„åˆ†å¤±è´¥ (APIå“åº”å¼‚å¸¸)")
                            else:
                                st.metric("ä¿¡åº¦ (Faithfulness)", f"{f_score:.2f}/10",
                                          help="æ£€æµ‹æ˜¯å¦å­˜åœ¨å¹»è§‰ï¼Œæ˜¯å¦å¿ äºåŸæ–‡")
                        with c2:
                            if r_score is None:
                                st.warning("ç›¸å…³åº¦: è¯„åˆ†å¤±è´¥ (APIå“åº”å¼‚å¸¸)")
                            else:
                                st.metric("ç›¸å…³åº¦ (Relevance)", f"{r_score:.2f}/10", help="å›ç­”æ˜¯å¦åˆ‡é¢˜")

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ è®ºæ–‡ï¼ˆæ”¯æŒå¤šç¯‡ï¼‰å¹¶è¾“å…¥ API Key")